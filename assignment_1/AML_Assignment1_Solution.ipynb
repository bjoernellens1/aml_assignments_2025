{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# AML Assignment 1 \u2014 Full Solution Notebook\n", "\n", "*Generated: 2025-11-12 20:40:03*\n", "\n", "This notebook provides a compact but complete solution for a standard first assignment in Applied Machine Learning (AML):\n", "\n", "1. **Linear Regression (closed\u2011form / MLE)** on the California Housing dataset (with an offline synthetic fallback if the dataset cannot be fetched).\n", "2. **Gradient Descent from scratch** for linear regression with Mean Squared Error, including a convergence plot.\n", "3. **Regularization**: Ridge and Lasso with standardization and simple cross\u2011validation.\n", "4. **Evaluation**: RMSE/MAE/$R^2$, residual analysis, and feature importance visualization.\n", "\n", "**Note on data:** If the California Housing dataset is not cached locally and the environment has no internet, the loader below transparently falls back to a synthetic regression dataset with the same number of samples and features. The rest of the pipeline remains identical.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 0. Setup\n", "We import the required libraries. All plots use **matplotlib** (one chart per figure, no custom colors) per the constraints."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import numpy as np\n", "import matplotlib.pyplot as plt\n", "from dataclasses import dataclass\n", "from typing import Tuple, Dict\n", "\n", "from sklearn.model_selection import train_test_split, KFold\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n", "from sklearn.linear_model import Ridge, Lasso\n", "from sklearn.datasets import fetch_california_housing, make_regression\n", "\n", "np.random.seed(42)\n", "plt.rcParams['figure.figsize'] = (7, 4)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Data Loading (California Housing with offline fallback)\n", "If California Housing is unavailable to fetch, we create a synthetic dataset of the same shape (20,640 samples, 8 features)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def load_california_or_synthetic(random_state: int = 42) -> Tuple[np.ndarray, np.ndarray, Dict]:\n", "    try:\n", "        data = fetch_california_housing()\n", "        X = data.data\n", "        y = data.target\n", "        meta = {\"source\": \"CaliforniaHousing\", \"feature_names\": list(data.feature_names)}\n", "        return X, y, meta\n", "    except Exception as e:\n", "        # Synthetic fallback: same n_samples and n_features\n", "        X, y = make_regression(n_samples=20640, n_features=8, noise=12.0, random_state=random_state)\n", "        # Rescale target roughly to typical CaliforniaHousing scale\n", "        y = (y - y.min()) / (y.max() - y.min()) * 5.0\n", "        meta = {\"source\": \"SyntheticFallback\", \"feature_names\": [f\"f{i}\" for i in range(8)]}\n", "        return X, y, meta\n", "\n", "X, y, meta = load_california_or_synthetic()\n", "meta\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Train/Validation/Test Split\n", "We use a 60/20/20 split. Targets are not standardized; features are standardized where needed (e.g., for regularization)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n", "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n", "X_train.shape, X_val.shape, X_test.shape\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Linear Regression (Closed\u2011Form / MLE)\n", "For the Gaussian noise model, maximizing the likelihood is equivalent to minimizing the MSE. The MLE solution is:\n", "$$\\hat{\\theta} = (X^\\top X)^{-1} X^\\top y,$$\n", "where $X$ includes a column of ones for the intercept. We implement this explicitly below."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def add_intercept(X: np.ndarray) -> np.ndarray:\n", "    return np.hstack([np.ones((X.shape[0], 1)), X])\n", "\n", "def closed_form_linreg(X: np.ndarray, y: np.ndarray) -> np.ndarray:\n", "    # Add intercept\n", "    Xb = add_intercept(X)\n", "    # Moore\u2013Penrose inverse for numerical stability\n", "    theta = np.linalg.pinv(Xb.T @ Xb) @ (Xb.T @ y)\n", "    return theta  # shape (d+1,)\n", "\n", "theta_hat = closed_form_linreg(X_train, y_train)\n", "theta_hat[:5], theta_hat.shape\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Evaluation Helper"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def predict_with_theta(theta: np.ndarray, X: np.ndarray) -> np.ndarray:\n", "    Xb = add_intercept(X)\n", "    return Xb @ theta\n", "\n", "def eval_regression(y_true, y_pred) -> dict:\n", "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n", "    mae = mean_absolute_error(y_true, y_pred)\n", "    r2 = r2_score(y_true, y_pred)\n", "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n", "\n", "pred_val_cf = predict_with_theta(theta_hat, X_val)\n", "pred_test_cf = predict_with_theta(theta_hat, X_test)\n", "metrics_cf_val = eval_regression(y_val, pred_val_cf)\n", "metrics_cf_test = eval_regression(y_test, pred_test_cf)\n", "metrics_cf_val, metrics_cf_test\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Residual Plot (Closed\u2011Form)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["residuals = y_val - pred_val_cf\n", "plt.figure()\n", "plt.scatter(pred_val_cf, residuals, s=6)\n", "plt.axhline(0)\n", "plt.xlabel('Predicted')\n", "plt.ylabel('Residual')\n", "plt.title('Residuals vs Predicted (Closed-form)')\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Gradient Descent for Linear Regression (from scratch)\n", "We optimize $\\theta$ by minimizing MSE with batch gradient descent. We report the loss across iterations and compare to the closed\u2011form solution."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def mse_loss(theta: np.ndarray, X: np.ndarray, y: np.ndarray) -> float:\n", "    y_pred = predict_with_theta(theta, X)\n", "    return float(np.mean((y_pred - y) ** 2))\n", "\n", "def gradient_descent_linreg(X: np.ndarray, y: np.ndarray, lr=1e-3, iters=2000):\n", "    Xb = add_intercept(X)\n", "    n, d = Xb.shape\n", "    theta = np.zeros(d)\n", "    history = []\n", "    for t in range(iters):\n", "        y_pred = Xb @ theta\n", "        grad = (2.0/n) * (Xb.T @ (y_pred - y))\n", "        theta -= lr * grad\n", "        if t % 10 == 0:\n", "            history.append(mse_loss(theta, X, y))\n", "    return theta, history\n", "\n", "theta_gd, hist = gradient_descent_linreg(X_train, y_train, lr=1e-3, iters=3000)\n", "pred_val_gd = predict_with_theta(theta_gd, X_val)\n", "metrics_gd_val = eval_regression(y_val, pred_val_gd)\n", "metrics_gd_val, len(hist)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Convergence Plot (MSE vs Iterations)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["plt.figure()\n", "plt.plot(np.arange(len(hist))*10, hist)\n", "plt.xlabel('Iteration')\n", "plt.ylabel('Training MSE')\n", "plt.title('Gradient Descent Convergence')\n", "plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Ridge and Lasso (with Standardization)\n", "We standardize features on the training set and apply the same transform to validation/test sets. Then we perform simple K\u2011fold CV (K=5) to select $\\alpha$ for Ridge and Lasso, and report performance."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["scaler = StandardScaler().fit(X_train)\n", "Xs_train = scaler.transform(X_train)\n", "Xs_val = scaler.transform(X_val)\n", "Xs_test = scaler.transform(X_test)\n", "\n", "def cv_model_and_alpha(Model, alphas, X, y, k=5):\n", "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n", "    alpha_scores = []\n", "    for a in alphas:\n", "        rmses = []\n", "        for tr, va in kf.split(X):\n", "            m = Model(alpha=a)\n", "            m.fit(X[tr], y[tr])\n", "            pred = m.predict(X[va])\n", "            rmses.append(mean_squared_error(y[va], pred, squared=False))\n", "        alpha_scores.append((a, float(np.mean(rmses))))\n", "    alpha_scores.sort(key=lambda t: t[1])\n", "    best_alpha = alpha_scores[0][0]\n", "    model = Model(alpha=best_alpha).fit(X, y)\n", "    return model, best_alpha, alpha_scores\n", "\n", "alphas = np.logspace(-3, 2, 12)\n", "ridge_model, ridge_alpha, ridge_scores = cv_model_and_alpha(Ridge, alphas, Xs_train, y_train)\n", "lasso_model, lasso_alpha, lasso_scores = cv_model_and_alpha(Lasso, alphas, Xs_train, y_train)\n", "ridge_alpha, lasso_alpha\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Ridge/Lasso Evaluation"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["pred_val_ridge = ridge_model.predict(Xs_val)\n", "pred_val_lasso = lasso_model.predict(Xs_val)\n", "metrics_ridge_val = eval_regression(y_val, pred_val_ridge)\n", "metrics_lasso_val = eval_regression(y_val, pred_val_lasso)\n", "\n", "pred_test_ridge = ridge_model.predict(Xs_test)\n", "pred_test_lasso = lasso_model.predict(Xs_test)\n", "metrics_ridge_test = eval_regression(y_test, pred_test_ridge)\n", "metrics_lasso_test = eval_regression(y_test, pred_test_lasso)\n", "\n", "metrics_ridge_val, metrics_lasso_val, metrics_ridge_test, metrics_lasso_test\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Coefficients (Magnitude) \u2014 Ridge vs Lasso\n", "We visualize coefficient magnitudes to illustrate regularization effects."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["feature_names = meta.get(\"feature_names\", [f\"f{i}\" for i in range(X.shape[1])])\n", "\n", "def bar_coeffs(model, title):\n", "    coefs = model.coef_\n", "    plt.figure()\n", "    plt.bar(np.arange(len(coefs)), np.abs(coefs))\n", "    plt.xticks(np.arange(len(coefs)), feature_names, rotation=45, ha='right')\n", "    plt.title(title)\n", "    plt.tight_layout()\n", "    plt.show()\n", "\n", "bar_coeffs(ridge_model, f'Ridge | alpha={ridge_alpha:.3g}')\n", "bar_coeffs(lasso_model, f'Lasso | alpha={lasso_alpha:.3g}')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Final Comparison on Test Set\n", "We compare closed\u2011form linear regression, gradient descent solution, Ridge, and Lasso on the test split."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["summary = {\n", "    'ClosedForm': metrics_cf_test,\n", "    'GradDescent': eval_regression(y_test, predict_with_theta(theta_gd, X_test)),\n", "    'Ridge': metrics_ridge_test,\n", "    'Lasso': metrics_lasso_test,\n", "}\n", "summary\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Notes & Takeaways\n", "- **Closed\u2011form** and **Gradient Descent** should agree closely if GD converged.\n", "- **Ridge** tends to reduce variance and often improves generalization when features are correlated.\n", "- **Lasso** encourages sparsity, potentially performing feature selection.\n", "- Always **standardize** features before L2/L1 regularization to keep coefficients on comparable scales.\n", "- Inspect **residuals** to validate assumptions (homoscedasticity, linearity) and identify failure modes."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.11"}}, "nbformat": 4, "nbformat_minor": 5}